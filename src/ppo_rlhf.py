import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
import numpy as np
from dataclasses import dataclass


@dataclass
class PPOConfig:
    lr: float = 2e-6
    kl_coef: float = 0.3
    clip_range: float = 0.1
    ppo_epochs: int = 8
    batch_size: int = 8
    max_new_tokens: int = 40
    entropy_coef: float = 0.0  # Kept for reference, but set to 0.0
    kl_threshold: float = 1.0  # early stop if KL too large
       



class PPOTrainer:
    def __init__(self, model_path: str, reward_fn):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # Policy & reference models
        self.policy = AutoModelForCausalLM.from_pretrained(model_path).to(self.device)
        self.ref = AutoModelForCausalLM.from_pretrained(model_path).to(self.device)
        for p in self.ref.parameters():
            p.requires_grad = False

        # Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        # ç¡®ä¿ pad_token è®¾ç½®
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.reward_fn = reward_fn
        self.cfg = PPOConfig()
        self.optimizer = optim.Adam(self.policy.parameters(), lr=self.cfg.lr)

    # Generation (Rollout)
    def generate(self, prompt):
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            out = self.policy.generate(
                **inputs,
                max_new_tokens=self.cfg.max_new_tokens,
                do_sample=True,
                top_p=0.9,
                temperature=0.7,
                repetition_penalty=1.2, # ADDED: Apply penalty during PPO sampling to fight repetition
                pad_token_id=self.tokenizer.eos_token_id
            )
        # è§£ç å¹¶å»é™¤ prompt éƒ¨åˆ†
        full = self.tokenizer.decode(out[0], skip_special_tokens=True)
        # ç®€å•å¤„ç†ï¼šæ‰¾åˆ° prompt ç»“æŸåçš„å†…å®¹ä½œä¸º response
        if full.startswith(prompt):
            return full[len(prompt):].strip()
        else:
            # å¦‚æœæ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿®æ”¹äº† promptï¼Œè¿”å›å…¨éƒ¨ç”Ÿæˆå†…å®¹ï¼ˆéæœ€ä½³åšæ³•ï¼Œä½†åº”å¯¹å¤æ‚åœºæ™¯ï¼‰
            return full.strip()

    
    # 1. ä¼˜åŒ– logprobs è®¡ç®—ï¼šæ”¹ä¸ºè¿”å› token-level çš„ mean (å¹³å‡å€¼)
    def compute_logprobs(self, model, input_ids, attention_mask, labels):
        """
        è®¡ç®—æ¨¡å‹åœ¨ç»™å®š labels ä¸‹çš„å¹³å‡ log-probabilityã€‚
        è¿”å›çš„æ˜¯ token-level çš„å¹³å‡å€¼ï¼Œè€Œä¸æ˜¯ per-sequence çš„æ€»å’Œã€‚
        """
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        
        # Shift logits and labels for language modeling
        shift_logits = logits[:, :-1, :]
        shift_labels = labels[:, 1:]
        
        # CrossEntropyLoss é»˜è®¤è®¡ç®— Negative Log Likelihood (NLL)
        loss_fct = nn.CrossEntropyLoss(reduction='none')
        # æ‹‰å¹³å¼ é‡è¿›è¡Œè®¡ç®—
        loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))
        # è¿˜åŸå½¢çŠ¶ [Batch_Size, Sequence_Length - 1]
        loss = loss.view(shift_labels.size())
        
        # æ©ç ï¼šåªè®¡ç®—é -100 çš„ token (å³ response token)
        mask = (shift_labels != -100).float()
        
        # å“åº”é•¿åº¦ (é -100 token çš„æ•°é‡)
        response_lengths = mask.sum(dim=1)
        # é˜²æ­¢é™¤ä»¥é›¶
        response_lengths[response_lengths == 0] = 1 
        
        # Sequence NLL: (loss * mask).sum(dim=1)
        sequence_nll = (loss * mask).sum(dim=1)
        
        # Sequence Logprob (Mean): -NLL_sum / N_tokens
        sequence_logprobs = -sequence_nll / response_lengths
        
        # å¦‚æœè¾“å…¥æ˜¯ [1, L]ï¼Œåˆ™è¿”å› size [1] çš„ tensor
        return sequence_logprobs
    
    # 2. PPO Step (é‡æ„ä¸ºæ‰¹å¤„ç†ï¼Œå¹¶å¼•å…¥ Advantage Normalization)
    def ppo_step(self, prompts, responses, rewards, old_logprobs):
        batch_size = len(prompts)

        # å°† rewards å’Œ old_logprobs è½¬æ¢ä¸º tensor æ–¹ä¾¿æ‰¹å¤„ç†
        rewards = torch.tensor(rewards, device=self.device, dtype=torch.float)
        # old_logprobs æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œéœ€è¦åˆå¹¶æˆä¸€ä¸ª tensor
        old_logprobs_tensor = torch.cat(old_logprobs).to(self.device)
        
        # ç¡®ä¿ old_logprobs_tensor å½¢çŠ¶æ­£ç¡®
        if old_logprobs_tensor.dim() == 0:
             old_logprobs_tensor = old_logprobs_tensor.unsqueeze(0)

        for epoch in range(self.cfg.ppo_epochs):
            
            # --- 1. é‡æ–°è®¡ç®—å½“å‰ Logprobs, Ref Logprobs ---
            
            new_lps = []
            ref_lps = []
            
            # ç®€åŒ–èµ·è§ï¼Œè¿™é‡Œå¯ä»¥å†æ¬¡éå†ï¼Œä½†æ›´é«˜æ•ˆçš„åšæ³•æ˜¯ä½¿ç”¨ DataLoader æˆ– padding/truncation
            # é‰´äºå½“å‰ä»£ç ç»“æ„ï¼Œæˆ‘ä»¬å…ˆä¿æŒé€ä¸ªç¼–ç 
            for i in range(batch_size):
                p = prompts[i]
                r = responses[i]
                
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªå¤„ç†å•ä¸ªåºåˆ—ï¼Œæ‰€ä»¥æ— éœ€ padding
                text = p + r
                enc = self.tokenizer(text, return_tensors="pt").to(self.device)
                
                input_ids = enc.input_ids
                attention_mask = enc.attention_mask

                # Build labels (masking prompt and padding)
                labels = input_ids.clone()
                prompt_ids = self.tokenizer(p).input_ids
                prompt_len = len(prompt_ids)

                labels[:, :prompt_len] = -100
                labels[labels == self.tokenizer.pad_token_id] = -100
                
                # Compute logprobs (token-level mean)
                new_lp = self.compute_logprobs(self.policy, input_ids, attention_mask, labels)
                with torch.no_grad():
                    ref_lp = self.compute_logprobs(self.ref, input_ids, attention_mask, labels)
                    
                new_lps.append(new_lp)
                ref_lps.append(ref_lp)
            
            # å°†æ”¶é›†åˆ°çš„ logprobs åˆå¹¶æˆæ‰¹æ¬¡ tensor
            new_lps = torch.cat(new_lps).to(self.device)
            ref_lps = torch.cat(ref_lps).to(self.device)
            
            # --- 2. PPO æ ¸å¿ƒè®¡ç®— (æ‰¹å¤„ç†) ---
            
            # KL divergence (token-level mean)
            kl = new_lps - ref_lps 
            mean_kl = kl.mean()

            # KL early stopping
            if mean_kl.item() > self.cfg.kl_threshold:
                print(f"âš  KL too high ({mean_kl.item():.4f}), early stopping PPO epoch")
                break
                
            # Compute Advantage (Total Reward: R(x,y) - kl_coef * KL)
            non_score_reward = -self.cfg.kl_coef * kl
            advantages = rewards + non_score_reward
            
            # === [å…³é”®ä¿®æ”¹] Advantage Normalization ===
            if batch_size > 1:
                # æ ‡å‡†åŒ–: (Adv - Mean) / (Std + epsilon)
                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
            # PPO ratio
            ratio = torch.exp(new_lps - old_logprobs_tensor)
            
            # Clipped objective
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 
                                 1.0 - self.cfg.clip_range, 
                                 1.0 + self.cfg.clip_range) * advantages

            ppo_loss = -torch.min(surr1, surr2).mean() # Mean across the batch

            # Final Loss: åªä½¿ç”¨ PPO Loss (ç§»é™¤æœ‰è¯¯çš„ Entropy é¡¹)
            loss = ppo_loss

            # Optimization step
            self.optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)
            self.optimizer.step()
            
            print(f"[PPO] epoch {epoch+1}/{self.cfg.ppo_epochs}, avg loss={loss.item():.4f}, mean KL={mean_kl.item():.4f}")

    
    # Main Trainer
    
    def train(self, prompts):
        print(" Starting PPO RLHF training...\n")

        for start in range(0, len(prompts), self.cfg.batch_size):
            batch_prompts = prompts[start:start + self.cfg.batch_size]

            responses = []
            rewards = []
            old_logprobs = []

            # Rollout
            for p in batch_prompts:
                r = self.generate(p)
                responses.append(r)

                # Call the reward function with both prompt and response
                reward_tuple = self.reward_fn(prompt=p, response=r)
                # The first element is the final_reward
                rew = float(reward_tuple[0])
                rewards.append(rew)

                # Calculate old logprobs for PPO update
                text = p + r
                enc = self.tokenizer(text, return_tensors="pt").to(self.device)
                input_ids = enc.input_ids
                attention_mask = enc.attention_mask

                labels = input_ids.clone()
                # å†æ¬¡è·å– prompt token id é•¿åº¦ï¼Œç¡®ä¿ä¸€è‡´æ€§
                prompt_len = len(self.tokenizer(p).input_ids)
                labels[:, :prompt_len] = -100

                with torch.no_grad():
                    # ç°åœ¨ compute_logprobs è¿”å›çš„æ˜¯ token-level mean
                    lp = self.compute_logprobs(self.policy, input_ids, attention_mask, labels)

                old_logprobs.append(lp)

            # PPO step (ç°åœ¨æ”¯æŒæ‰¹å¤„ç†)
            self.ppo_step(batch_prompts, responses, rewards, old_logprobs)

        print("\nğŸ‰ Finished PPO RLHF training!")
